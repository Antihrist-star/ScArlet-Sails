"""
CORE - FEATURE ENGINE v2
Multi-timeframe feature engineering matching xgboost_normalized_model.json
31 features across 4 timeframes: 15m, 1h, 4h, 1d
"""
import logging
from dataclasses import dataclass
from typing import List, Sequence, Dict, Any

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import joblib

logger = logging.getLogger(__name__)


class FeatureEngine:
    """
    –°–æ–∑–¥–∞—ë—Ç multi-timeframe –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.

    Features (31 total):
    - 15m: 13 features (base timeframe)
    - 1h:  6 features (higher TF context)
    - 4h:  6 features (trend context)
    - 1d:  6 features (macro context)
    """

    def __init__(self, config):
        self.config = config
        self.normalize = config.get("features", {}).get("normalize", True)
        self.scaler = None

        # Feature list (matches model exactly)
        self.feature_names = [
            # 15m features (13)
            "15m_RSI_14",
            "15m_price_to_EMA9",
            "15m_price_to_EMA21",
            "15m_price_to_SMA50",
            "15m_BB_width_pct",
            "15m_ATR_pct",
            "15m_returns_5",
            "15m_returns_10",
            "15m_volume_ratio_5",
            "15m_volume_ratio_10",
            "15m_price_to_EMA9_dup",  # Kept for compatibility
            "15m_price_to_EMA21_dup",
            "15m_price_to_SMA50_dup",
            # 1h features (6)
            "1h_RSI_14",
            "1h_returns_5",
            "1h_price_to_EMA9",
            "1h_price_to_EMA21",
            "1h_price_to_SMA50",
            "1h_ATR_pct",
            # 4h features (6)
            "4h_RSI_14",
            "4h_returns_5",
            "4h_price_to_EMA9",
            "4h_price_to_EMA21",
            "4h_price_to_SMA50",
            "4h_ATR_pct",
            # 1d features (6)
            "1d_RSI_14",
            "1d_returns_5",
            "1d_price_to_EMA9",
            "1d_price_to_EMA21",
            "1d_price_to_SMA50",
            "1d_ATR_pct",
        ]

        logger.info(
            f"FeatureEngine v2 –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω. "
            f"Features: {len(self.feature_names)}, Normalize: {self.normalize}"
        )

    def calculate_features(self, df_15m, df_1h=None, df_4h=None, df_1d=None):
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç multi-timeframe –ø—Ä–∏–∑–Ω–∞–∫–∏.

        Args:
            df_15m: DataFrame —Å OHLCV –¥–ª—è 15m (required)
            df_1h: DataFrame —Å OHLCV –¥–ª—è 1h (optional)
            df_4h: DataFrame —Å OHLCV –¥–ª—è 4h (optional)
            df_1d: DataFrame —Å OHLCV –¥–ª—è 1d (optional)

        Returns:
            DataFrame —Å 31 –ø—Ä–∏–∑–Ω–∞–∫–æ–º
        """

        # Base features from 15m
        features_15m = self._calculate_timeframe_features(df_15m, "15m", full=True)

        # Higher timeframe features (if available)
        if df_1h is not None:
            features_1h = self._calculate_timeframe_features(df_1h, "1h", full=False)
            # Resample to 15m frequency
            features_1h = self._resample_to_15m(features_1h, df_15m.index)
        else:
            # Fill with neutral values if not available
            features_1h = pd.DataFrame(index=df_15m.index)
            for col in [f for f in self.feature_names if f.startswith("1h_")]:
                features_1h[col] = 0.0

        if df_4h is not None:
            features_4h = self._calculate_timeframe_features(df_4h, "4h", full=False)
            features_4h = self._resample_to_15m(features_4h, df_15m.index)
        else:
            features_4h = pd.DataFrame(index=df_15m.index)
            for col in [f for f in self.feature_names if f.startswith("4h_")]:
                features_4h[col] = 0.0

        if df_1d is not None:
            features_1d = self._calculate_timeframe_features(df_1d, "1d", full=False)
            features_1d = self._resample_to_15m(features_1d, df_15m.index)
        else:
            features_1d = pd.DataFrame(index=df_15m.index)
            for col in [f for f in self.feature_names if f.startswith("1d_")]:
                features_1d[col] = 0.0

        # Combine all features
        features = pd.concat(
            [features_15m, features_1h, features_4h, features_1d], axis=1
        )

        # Reorder to match model
        features = features[self.feature_names]

        # Fill NaN
        features = features.fillna(method="ffill").fillna(0)

        # Normalize if enabled
        if self.normalize:
            features = self._normalize_features(features)

        logger.info(f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {len(features.columns)}")

        return features

    def _calculate_timeframe_features(self, df, tf_prefix, full=False):
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞.

        Args:
            df: OHLCV DataFrame
            tf_prefix: Prefix –¥–ª—è –∫–æ–ª–æ–Ω–æ–∫ ('15m', '1h', etc)
            full: –ï—Å–ª–∏ True, —Å–æ–∑–¥–∞—ë—Ç –≤—Å–µ 13 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è 15m. –ï—Å–ª–∏ False, —Ç–æ–ª—å–∫–æ 6.
        """
        df = df.copy()
        features = pd.DataFrame(index=df.index)

        # RSI
        features[f"{tf_prefix}_RSI_14"] = self._calculate_rsi(df["close"], 14)

        # Returns
        features[f"{tf_prefix}_returns_5"] = df["close"].pct_change(5)

        # Price to EMAs/SMA
        ema9 = df["close"].ewm(span=9, adjust=False).mean()
        ema21 = df["close"].ewm(span=21, adjust=False).mean()
        sma50 = df["close"].rolling(50).mean()

        features[f"{tf_prefix}_price_to_EMA9"] = (df["close"] - ema9) / ema9
        features[f"{tf_prefix}_price_to_EMA21"] = (df["close"] - ema21) / ema21
        features[f"{tf_prefix}_price_to_SMA50"] = (df["close"] - sma50) / sma50

        # ATR
        features[f"{tf_prefix}_ATR_pct"] = self._calculate_atr(df) / df["close"]

        if full:  # 15m gets extra features
            # Returns 10
            features[f"{tf_prefix}_returns_10"] = df["close"].pct_change(10)

            # Bollinger Bands width
            sma20 = df["close"].rolling(20).mean()
            std20 = df["close"].rolling(20).std()
            bb_width = (std20 * 2) / sma20
            features[f"{tf_prefix}_BB_width_pct"] = bb_width

            # Volume ratios
            vol_ma5 = df["volume"].rolling(5).mean()
            vol_ma10 = df["volume"].rolling(10).mean()
            features[f"{tf_prefix}_volume_ratio_5"] = df["volume"] / vol_ma5
            features[f"{tf_prefix}_volume_ratio_10"] = df["volume"] / vol_ma10

            # Duplicates (for compatibility)
            features[f"{tf_prefix}_price_to_EMA9_dup"] = features[
                f"{tf_prefix}_price_to_EMA9"
            ]
            features[f"{tf_prefix}_price_to_EMA21_dup"] = features[
                f"{tf_prefix}_price_to_EMA21"
            ]
            features[f"{tf_prefix}_price_to_SMA50_dup"] = features[
                f"{tf_prefix}_price_to_SMA50"
            ]

        return features

    def _calculate_rsi(self, series, period=14):
        """Calculate RSI"""
        delta = series.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi

    def _calculate_atr(self, df, period=14):
        """Calculate ATR"""
        high_low = df["high"] - df["low"]
        high_close = np.abs(df["high"] - df["close"].shift())
        low_close = np.abs(df["low"] - df["close"].shift())

        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = ranges.max(axis=1)

        atr = true_range.rolling(period).mean()
        return atr

    def _resample_to_15m(self, df, target_index):
        """Resample higher timeframe to 15m frequency"""
        df_resampled = df.reindex(target_index, method="ffill")
        return df_resampled

    def _normalize_features(self, features):
        """Normalize features using StandardScaler"""
        if self.scaler is None:
            self.scaler = StandardScaler()
            features_scaled = self.scaler.fit_transform(features)
            logger.info("–ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã (–Ω–æ–≤—ã–π scaler)")
        else:
            features_scaled = self.scaler.transform(features)
            logger.info("–ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã (—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π scaler)")

        return pd.DataFrame(
            features_scaled,
            index=features.index,
            columns=features.columns,
        )

    def save_scaler(self, path):
        """Save scaler to file"""
        if self.scaler is not None:
            joblib.dump(self.scaler, path)
            logger.info(f"  üíæ Scaler saved: {path}")

    def load_scaler(self, path):
        """Load scaler from file"""
        self.scaler = joblib.load(path)
        logger.info(f"  üìÇ Scaler loaded: {path}")


# =======================
# FeatureSpecV3
# =======================
from dataclasses import dataclass
from typing import List, Sequence, Dict, Any


@dataclass
class FeatureSpecV3:
    """
    Lightweight feature specification for Model 2 (XGBoost v3).

    - feature_names: —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–±–µ–∑ target –∏ —Å–ª—É–∂–µ–±–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫)
    - target_column: –∏–º—è —Ç–∞—Ä–≥–µ—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'target')

    –ù–∞—à–∞ –∑–∞–¥–∞—á–∞:
    - –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω–æ –≤—ã—Ç–∞—â–∏—Ç—å —Ñ–∏—á–∏ –∏–∑ train_df
    - –∑–∞—Ç–µ–º —á–µ—Ä–µ–∑ enforce() –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–æ—Ç –∂–µ –Ω–∞–±–æ—Ä/–ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –≤ val/test.
    """

    feature_names: List[str]
    target_column: str = "target"

    @classmethod
    def from_dataframe(
        cls,
        df: pd.DataFrame,
        target_column: str = "target",
    ) -> "FeatureSpecV3":
        """
        –°—Ç—Ä–æ–∏–º spec –ø–æ train_df:
        - –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º target –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (raw_ret, fee_ret, rapnl)
        - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –∫–∞–∫ –≤ train_df
        """
        exclude = {target_column, "raw_ret", "fee_ret", "rapnl"}
        feature_names = [c for c in df.columns if c not in exclude]
        return cls(feature_names=feature_names, target_column=target_column)

    @property
    def n_features(self) -> int:
        return len(self.feature_names)

    def enforce(
        self,
        df: pd.DataFrame,
        raise_on_missing: bool = True,
    ) -> pd.DataFrame:
        """
        –ü—Ä–∏–≤–æ–¥–∏–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –∫:
        - —Ç–µ–º –∂–µ —Ñ–∏—á–∞–º
        - –≤ —Ç–æ–º –∂–µ –ø–æ—Ä—è–¥–∫–µ

        Target –∏ –ª–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –≤—ã–±—Ä–∞—Å—ã–≤–∞—é—Ç—Å—è.
        """
        missing = [c for c in self.feature_names if c not in df.columns]
        if missing and raise_on_missing:
            raise KeyError(f"Missing required features: {missing}")

        # –¢–æ–ª—å–∫–æ —Ñ–∏—á–∏, –≤ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–º –ø–æ—Ä—è–¥–∫–µ
        return df.loc[:, self.feature_names].copy()

    def validate(self, actual_columns: Sequence[str]) -> Dict[str, Any]:
        """
        –°—Ä–∞–≤–Ω–∏—Ç—å –æ–∂–∏–¥–∞–µ–º—ã–µ —Ñ–∏—á–∏ —Å —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º —Å–ø–∏—Å–∫–æ–º –∫–æ–ª–æ–Ω–æ–∫.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict:
          - expected: —Å–ø–∏—Å–æ–∫ –æ–∂–∏–¥–∞–µ–º—ã—Ö —Ñ–∏—á
          - actual: —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫
          - missing: —Ñ–∏—á–∏, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ actual
          - extra: –ª–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ expected
          - misordered: —Ñ–∏—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å, –Ω–æ —Å—Ç–æ—è—Ç –Ω–µ –Ω–∞ —Å–≤–æ–∏—Ö –º–µ—Å—Ç–∞—Ö
          - reordered: True, –µ—Å–ª–∏ –ø–æ—Ä—è–¥–æ–∫ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è, –Ω–æ –Ω–∞–±–æ—Ä —Ñ–∏—á —Ç–æ—Ç –∂–µ
          - is_ok: True, –µ—Å–ª–∏ –≤—Å—ë —Å–æ–≤–ø–∞–¥–∞–µ—Ç (–Ω–µ—Ç missing/extra/misordered)
        """
        actual = list(actual_columns)
        expected = list(self.feature_names)

        expected_set = set(expected)
        actual_set = set(actual)

        missing = [c for c in expected if c not in actual_set]
        extra = [c for c in actual if c not in expected_set]

        # –§–∏—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –∏ —Ç–∞–º, –∏ —Ç–∞–º
        shared = [c for c in expected if c in actual_set]

        # misordered = —Ñ–∏—á–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç, –Ω–æ —Å—Ç–æ—è—Ç –Ω–µ –Ω–∞ —Å–≤–æ–∏—Ö –º–µ—Å—Ç–∞—Ö
        misordered = [
            c for c in shared
            if actual.index(c) != expected.index(c)
        ]

        # reordered = —á–∏—Å—Ç–æ –ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–µ–Ω–æ (–Ω–µ—Ç missing/extra, –Ω–æ –µ—Å—Ç—å misordered)
        reordered = bool(misordered) and not missing and not extra

        is_ok = not missing and not extra and not misordered

        return {
            "expected": expected,
            "actual": actual,
            "missing": missing,
            "extra": extra,
            "misordered": misordered,
            "reordered": reordered,
            "is_ok": is_ok,
        }
